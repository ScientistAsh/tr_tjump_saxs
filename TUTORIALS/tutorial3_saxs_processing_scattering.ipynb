{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05698196-cbe0-4693-bbe3-cecd0303f1ff",
   "metadata": {},
   "source": [
    "# Tutorial 3: Data Processing for Time Resolved, Temperature-Jump SAXS Scattering Curves\n",
    "\n",
    "**Package Information:**<br>\n",
    "Currently the [tr_tjump_saxs](https://gitlab.oit.duke.edu/tr_t-jump_saxs/y22-23/-/tree/main?ref_type=heads \"tr_tjump_saxs\") package only works through the Python3 command line. The full dependencies can be found on the [Henderson GitLab Page](https://gitlab.oit.duke.edu/tr_t-jump_saxs/y22-23/-/tree/main?ref_type=heads \"tr_tjump_saxs\") and the environment can be cloned from the [environment.yml file](https://gitlab.oit.duke.edu/tr_t-jump_saxs/y22-23/-/blob/main/environment.yml?ref_type=heads \"environment.yml file\"). The data analysis can be executed from an interactive Python command line such as [iPython](https://www.python.org/) or [Jupyter](https://jupyter.org/) or the code can be written in a script to run in a non-interactive mode. The preferred usage is in Jupyter Lab as this is the environment the package was developed in. Jupyter also provides a file where all code, output of code, and notes can be contained in a single file and serves a record of the data analysis performed, the code used to conduct the data analysis, and the output of the analysis. \n",
    "\n",
    "**Tutorial Information:**<br>\n",
    "This set of tutorial notebooks will cover how to use the `tr_tjump_saxs` package to analyze TR, T-Jump SAXS data and the <a href=\"https://www.science.org/doi/10.1126/sciadv.adj0396\">workflow used to study HIV-1 Envelope glycoprotein dynamics. </a> This package contains multiple modules, each containing a set of functions to accomplish a specific subtask of the TR, T-Jump SAXS data analysis workflow. Many of the functions are modular and some can be helpful for analyzing static SAXS and other data sets as well. \n",
    "\n",
    "**Package Modules:**<br>\n",
    "> 1. `file_handling`<br>\n",
    "> 2. `saxs_processing`<br>\n",
    "> 3. `saxs_qc`<br>\n",
    "> 4. `saxs_kinetics`<br>\n",
    "> 5. `saxs_modeling`<br>\n",
    "\n",
    "**Developer:** [@ScientistAsh](https://github.com/ScientistAsh \"ScientistAsh GitHub\")\n",
    "\n",
    "**Updated:** 2 February 2024\n",
    "\n",
    "# Tutorial 3 Introduction\n",
    "In the Tutorial 3 + 4 notebooks, I introduce the `saxs_processing` module from the `tr_tjump_saxs` package. In this Tutorial 3 Notebook I will cover the processing of T-Jump scattering curves. \n",
    "\n",
    "The `saxs_processing` module provides functions that will find and remove outliers, scale curves, and subtract curves. This tutorial assumes that you have finished the first two tutorials. If you find any issues with this tutorial or module, please create an issue on the repository GitLab page ([tr_tjump_saxs issues](https://gitlab.oit.duke.edu/tr_t-jump_saxs/y22-23/-/issues \"tr_tjump_saxs Issues\")). \n",
    "\n",
    "# Module functions:\n",
    "> 1. `svd_outliers()` uses an SVD method to determine outliers in scattering curves. <br>\n",
    "> 2. `iterative_chi()` uses a $\\chi^2$ method to determine outliers in T-Jump difference curves . <br>\n",
    "> 3. `saxs_scale()` scales a given curve to a given reference curve. <br>\n",
    "> 4. `saxs_sub()` subtracts a given curve from a reference curve. <br>\n",
    "> 5. `auc_outliers()` will determine the outliers of a given data set using an area under the curve the method.$^*$ <br>\n",
    "> 6. `move_outliers()` will move the outlier files to a given directory for quarantine. This is not recommended because files can get lost easily.$^*$ <br>\n",
    "\n",
    "$^*$Please note that the `move_outliers` and `auc_outliers` functions are no longer used and will not be maintained and not recommended for use\n",
    "\n",
    "## Tutorial Files:\n",
    "\n",
    "### Data Files\n",
    "The original data used in this analysis is deposited on the [SASBDB](https://www.sasbdb.org/) with accession numbers:\n",
    "> **Static Data:** <br>\n",
    "    - *CH505 Temperature Sereies*: SASDT29, SASDT39, SASDT49, SASDT59 <br>\n",
    "    - *CH848 Temperature Series*: SASDTH9, SASDTJ9, SASDTK9, SASDTL9 <br>\n",
    "<br>\n",
    "> **T-Jump Data:** <br>\n",
    "    - *CH505 T-Jump Data*: SASDT69, SASDT79, SASDT89, SASDT99, SASDTA9, SASDTB9, SASDTC9, SASDTD9, SASDTE9, SASDTF9, SASDTG9 <br>\n",
    "     - *CH848 T-Jump Data*: SASDTM9, SASDTN9, SASDTP9, SASDTQ9, SASDTR9, SASDTS9, SASDTT9, SASDTU9, SASDTV9, SASDTW9 <br>\n",
    "<br>\n",
    "> **Static Env SOSIP Panel:** SASDTZ9, SASDU22, SASDU32, SASDU42, SASDTX9, SASDTY9 <br>\n",
    "\n",
    "Additional MD data associated with the paper can be found on [Zenodo](https://zenodo.org/records/10451687).\n",
    "\n",
    "### Output Files\n",
    "Example output is included in the [OUTPUT](https://github.com/ScientistAsh/tr_tjump_saxs/tree/main/TUTORIALShttps://github.com/ScientistAsh/tr_tjump_saxs/tree/main/TUTORIALS/OUTPUT/) subdirectory in the [TUTORIALS](https://gitlab.oit.duke.edu/tr_t-jump_saxs/y22-23/-/tree/main/TUTORIALS?ref_type=heads) directory.  \n",
    "\n",
    "# How to Use Jupyter Notebooks\n",
    "You can execute the code directly in this notebook or create your own notebook and copy the code there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0a0b62-afdc-4f77-ab79-93618c4d440c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "    <b><i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>&nbsp; Tips</b><br>\n",
    "    \n",
    "    <b>1.</b> To run the currently highlighted cell, hit the <code>shift</code> and <code>enter</code> keys at the same time.<br>\n",
    "    <b>2</b>. To get help with a specific function, place the cursor in the functions brackets and hit the <code>shift</code> and <code>tab</code> keys at the same time.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6d2ab2-073b-4b86-bcbc-ffe4d0a11387",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"background-color: white; border: 2px solid; padding: 10px\">\n",
    "    <b><i class=\"fa fa-star\" aria-hidden=\"true\"></i>&nbsp; In the Literature</b><br>\n",
    "    \n",
    "    Our <a href=\"https://www.biorxiv.org/content/10.1101/2023.05.17.541130v1\">recent paper </a> on BioArxiv provides an example of the type of data, the analysis procedure, and example output for this type of data analysis.  <br> \n",
    "    \n",
    "    <p style=\"text-align:center\">\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e936c595-a9f0-4387-9a6b-ba7d74e7a337",
   "metadata": {},
   "source": [
    "# Import Modules\n",
    "\n",
    "In order to use the `saxs_processing` module, the `tr_tjump_saxs` package needs to be imported. The dependecies will automatically be imported with the package import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c142b14-d63c-4ff3-805f-ded0030bbe71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import sys to allow python to use the file browser to find files\n",
    "import sys\n",
    "\n",
    "# append the path for the tr_tjump_saxs_analysis package to the PYTHONPATH\n",
    "sys.path.append(r'../')\n",
    "\n",
    "# import CH505TF_SAXS analysis dependent packages and custom functions\n",
    "from file_handling import *\n",
    "from saxs_processing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561aed21-e782-4394-852a-6ba7a86c9743",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b><i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>&nbsp; Tips</b><br>\n",
    "    Be sure that the path for the <code>tr_tjump_saxs</code> package appended to the <code>PYTHONPATH</code> matches the path to the repository on your machine.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517c4e29-2620-4125-bfce-d0082dcef92d",
   "metadata": {},
   "source": [
    "<a id='Overview'></a>\n",
    "\n",
    "# Overview: Finding SAXS Scattering Outlier Curves\n",
    "\n",
    "The first step in analyzing TR, T-Jump SAXS data is to detect and remove outlier scattering and difference curves. During a TR, T-Jump collection, scattering curves are measured for both buffer and protein. Static SAXS scattering curves are collected in addition to \"laser off\" and \"laser on\" T-Jump scattering curves for both protein and buffer T-Jumps. The outliers for all of these sets of scattering curves needs to be determined before further analysis. \n",
    "\n",
    "In this tutorial, we will only show this analysis on the T-Jump scattering curves. Determining outliers for T-Jump scattering curves is more complicated than for the static sets because the outliers have to be determined separately for \"laser on\" curves and \"laser off\" curves, requiring a multi-step procedure. Additionally, the \"laser off\" outlier curves must also be removed from the \"laser on\" curves (so if curve #25 is a \"laser off\" curve outlier then curve #25 must be removed from the \"laser on\" set even if it is not determined as an outlier from the \"laser on\" data set). \n",
    "\n",
    "# Example 1: Basic Usage\n",
    "\n",
    "## Step 1: Load Data\n",
    "\n",
    "### Step 1.1: Load Curves\n",
    "\n",
    "Before we can process SAXS data, we need to load the data using the functions in the `file_handling` module covered in [Tutorial 1](https://gitlab.oit.duke.edu/tr_t-jump_saxs/y22-23/-/blob/main/TUTORIALS/tutorial1_file-handling.ipynb?ref_type=heads \"Tutorial 1\"). We will use the `load_set()` function to load an entire set of curves and see how the outlier, scaling, and subtraction functions work with this example data set. If you have questions about loading data, slicing through the data arrays, or looping over multiple data sets, see [Tutorial 1](https://gitlab.oit.duke.edu/tr_t-jump_saxs/y22-23/-/blob/main/TUTORIALS/tutorial1_file-handling.ipynb?ref_type=heads \"Tutorial 1\").\n",
    "\n",
    "Because we need to analyze both the \"laser off\" and \"laser on\" T-Jump scattering curves, we need to load both sets of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370f18ff-d263-493f-8153-4f4096066b54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# Get all laser on scattering curves\n",
    "on_files = make_flist(directory='../../../TR_T-jump_SAXS_July2022/protein_20hz_set01/processedb/', \n",
    "                        prefix='protein_20hz_set01_1ms_', suffix='_Q.chi')\n",
    "\n",
    "# Get all -10us scattering curves from 20 Hz set 3 CH505TF Data\n",
    "off_files = make_flist(directory='../../../TR_T-jump_SAXS_July2022/protein_20hz_set01/processedb/',\n",
    "                        prefix='protein_20hz_set01_-5us_', suffix='_Q.chi')\n",
    "\n",
    "# sort files lists\n",
    "on_files.sort()\n",
    "off_files.sort()\n",
    "    \n",
    "\n",
    "# load laser on scattering curves\n",
    "on_data, on_array, q, on_err = load_set(flist=on_files, delim=' ', mask=10, err=False)\n",
    "\n",
    "# load laser off scattering curves\n",
    "off_data, off_array, q, off_err = load_set(flist=off_files, delim=' ', mask=10, err=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3508a96-f658-49b4-af81-a72f09026a4f",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "    <i class=\"fa fa-exclamation-triangle\"></i>&nbsp; <b>Check your PATH </b><br>\n",
    "    Make sure that the input and output directories match the PATH on your machine. Also be sure that the file prefixes and suffixes match what is used for your files. Edit the PATH varaibles for your machine. \n",
    "    </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b9749b-32f3-453b-8695-587f08e13811",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "    <i class=\"fa fa-exclamation-triangle\"></i>&nbsp; <b>Check your data structure</b><br>\n",
    "    Your data may be stored differently and it is important to make sure you understand your data structure before beginning any analysis. It is always a good idea to practice slicing and plotting your data set to be sure you understand the data structure once it is loaded.\n",
    "    </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1960fb4b-f20b-45ed-8948-e229415aece2",
   "metadata": {},
   "source": [
    "### Step 1.2: Check Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931249d5-d88f-40e8-889a-07129e3c414d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check length of array\n",
    "print(len(on_array))\n",
    "print(len(off_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffe606c-12d7-4174-99d3-48a45bc8e6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check shape of array\n",
    "print(on_array.shape)\n",
    "print(off_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0de551b-3e58-490f-b810-98a6c1189344",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# show first 5 files\n",
    "on_files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d208543f-486c-41eb-b216-f889d298f0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show first 5 files\n",
    "off_files[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e815ed-1f51-427b-bd94-59eff59f6f08",
   "metadata": {},
   "source": [
    "The array containing our data has length 250, which corresponds to the 250 different scattering curves collected for this time delay. Each of the 250 curves has 1908 points as indicated by the `shape()` function. The data has the correct structure to proceed with processing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2469bd55-3074-41fb-a44f-bb81176609bf",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "    <i class=\"fa fa-exclamation-triangle\"></i>&nbsp; <b>Overwritting Variables</b><br>\n",
    "    Note that the data array is overwritten with each iteration of the loop. \n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3125940-542d-4c46-9dc7-dd857db80058",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "    <i class=\"fa fa-exclamation-triangle\"></i>&nbsp; <b>Iterating</b><br>\n",
    "    <code>for</code> loops are not the most efficient implementation in Python and other methods of iteration may be faster. \n",
    "    </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cbe585-0d0a-4993-a16e-2ff543a4bf8e",
   "metadata": {},
   "source": [
    "### Step 1.3: Check Loaded curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ae58c0-a738-49dd-b4b2-73fef82ef498",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "on_labs = []\n",
    "off_labs = []\n",
    "\n",
    "for i,j in zip(on_files, off_files):\n",
    "    on_labs.append(i[-9:-6])\n",
    "    off_labs.append(j[-9:-6])\n",
    "    \n",
    "plot_curve(data_arr=on_array, q_arr=q, labels=on_labs, qmin=0.02, qmax=0.15, \n",
    "           imin=None, imax=None, x='Scattering Vector Å $^{-1}$', y='Scattering Intensity',\n",
    "           title='CH505 TR, T-Jump Scattering Curves for 1ms', save=True, save_dir='./OUTPUT/TUTORIAL3/PLOTS/', \n",
    "           save_name='tutorial3_ex1_step1.3a.png')\n",
    "\n",
    "plot_curve(data_arr=off_array, q_arr=q, labels=off_labs, qmin=0.02, qmax=0.15, \n",
    "           imin=None, imax=None, x='Scattering Vector Å $^{-1}$', y='Scattering Intensity',\n",
    "           title='CH505 TR, T-Jump Scattering Curves for -5$\\mu$s', save=True, save_dir='./OUTPUT/TUTORIAL3/PLOTS/', \n",
    "           save_name='tutorial3_ex1_step1.3b.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b73350c-77e8-4d26-9c36-d36d9f015ba2",
   "metadata": {},
   "source": [
    "Both the \"laser on\" and \"laser off\" curves plotted above look reasonable. We can now move on to outlier detection on the \"laser on\" and \"laser off\" scattering curves. \n",
    "\n",
    "## Step 2: Detecting Outliers in SAXS Scattering Curves\n",
    "\n",
    "The `tr_tjump_saxs` package uses a singular value decomposition (SVD) method to determine outliers in scattering curves (in addition to the previously mentioned pre-print, see also [Thompson et al](https://pubmed.ncbi.nlm.nih.gov/31527847/https://pubmed.ncbi.nlm.nih.gov/31527847/ \"Thompson et al\")).\n",
    "\n",
    "The `svd_outliers()` function will perform this analysis. `svd_outliers()` has 6 input parameters.\n",
    "\n",
    "> 1. The `arr` parameter indicates the data array containing the curves to run the outlier analysis on and is required input.<br>\n",
    "> 2. The `flist` parameter indicates the file list associated with the indicated data array and is required input.<br>\n",
    "> 3. `q` indicates the array containing the scattering vectors and is a required input.<br>\n",
    "> 4. The `cutoff` parameter indicates the threshold for outlier detection in xSTD and is an optional parameter with the default value  2.5x the standard deviation.<br>\n",
    "> 5. The `save_dir` and `save_name` parameters indicate the directory to save output in and the file name for output data, respectively, and are optional parameters. <br>\n",
    "\n",
    "The function returns a list of outlier files and a list containing the list index associated with the outlier files.\n",
    "\n",
    "There are no custom errors raised by the `svd_outliers()` function. For any errors raised, refer to the documentation for the function indicated in the traceback. \n",
    "\n",
    "### Step 2.1: Initial Outlier Detection\n",
    "As metioned in the [Overview](#Overview), outlier detection on T-Jump scattering curves is a multi-step process. First, an initial SVD outlier detection is run using all the collected curves. Outlier detection is run separately for \"laser on\" and \"laser off\" curves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd431611-e0cb-452c-b703-05788a4d5fed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#SVD outlier detection iteration 1\n",
    "print('Running SVD 1...')\n",
    "on_outlier_files1, on_outlier_index1 = svd_outliers(arr=on_array, flist=on_files, q=q, cutoff=2.5, \n",
    "                                                    save_dir='./OUTPUT/TUTORIAL3/OUTLIERS/', \n",
    "                                                    save_name='1ms_svd_outliers1')\n",
    "\n",
    "off_outlier_files1, off_outlier_index1 = svd_outliers(arr=off_array, flist=off_files, q=q, cutoff=2.5, \n",
    "                                                        save_dir='./OUTPUT/TUTORIAL3/OUTLIERS/', \n",
    "                                                        save_name='-5us_svd_outliers1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfc5ede-997b-4afa-8a05-5c55e422f8d6",
   "metadata": {},
   "source": [
    "### Step 2.2: Initial Outlier Removal\n",
    "\n",
    "The `svd_outliers()` function only detects outliers, it does not remove them from the file list. \n",
    "\n",
    "The `remove_outliers()` function removes previously determined outliers from the indicated file list. There are 3 input parameters for this function:\n",
    "> 1. The `flist` and `olist` parameters indicate the file list to remove outliers from and the list containing the outlier files, respectively. <br>\n",
    "> 2. The `fslice` parameter is optional and indicates how the input file names should be sliced to produce the output file name. \n",
    "\n",
    "The function returns the set of cleaned files as a list and the outlier list. \n",
    "\n",
    "There are no custom errors raised by the `remove_outliers()` function. For any raised errors refer to the docs for the function indicated in the traceback. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd429e3c-046a-463a-880d-121102dcb291",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove SVD iteration 1 outliers from file lists\n",
    "on_cleaned, on_outliers = remove_outliers(flist=on_files, olist=on_outlier_files1, fslice=[-9,-6])        \n",
    "off_cleaned, off_outliers = remove_outliers(flist=off_files, olist=off_outlier_files1, fslice=[-9,-6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e74fe2-c308-4114-b94e-6c4d4848dc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "on_cleaned[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63645676-3a5a-4ea1-9a19-e67f21a1c7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "on_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00482bdd-d0c9-4e9d-82cb-9cfb75ef60bd",
   "metadata": {},
   "source": [
    "### Step 2.3: Load Cleaned Curves\n",
    "Now that the first round of outliers have been removed, we need to load the cleaned file lists into new data arrays so that the outliers are also removed from the data arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe3a205-23f8-4f99-b847-18ca8ed21d4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load clean curves\n",
    "on_data, on_array, q, on_err = load_set(flist=on_cleaned, delim=' ', mask=10, err=False)\n",
    "off_data, off_array, q, off_err = load_set(flist=off_cleaned, delim=' ', mask=10, err=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575fafb4-83f5-47bd-8117-fee45b7a461e",
   "metadata": {},
   "source": [
    "### Step 2.4: Outlier Detection\n",
    "Outlier detection is determined relative to the average, yet the presence of outliers can skew the average value. If outliers are found in the initial outlier detection step, then it is best practice to run a second outlier detection to ensure that no outliers were missed due to a skewed value for the average. The procedure is the same as for the intial round of outlier detection. This is not necessary for this data set, but we will practice it here anyways. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2f08e6-8b6c-4413-8ff7-59d5c3acb98e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#SVD outlier detection iteration 2\n",
    "print('Running SVD 1...')\n",
    "on_outlier_files2, on_outlier_index2 = svd_outliers(arr=on_array, flist=on_cleaned, q=q, cutoff=2.5, \n",
    "                                                    save_dir='./OUTPUT/TUTORIAL3/OUTLIERS/', \n",
    "                                                    save_name='1ms_svd_outliers2')\n",
    "\n",
    "off_outlier_files2, off_outlier_index2 = svd_outliers(arr=off_array, flist=off_cleaned, q=q, cutoff=2.5, \n",
    "                                                        save_dir='./OUTPUT/TUTORIAL3/OUTLIERS/', \n",
    "                                                        save_name='-5us_svd_outliers2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea76578b-b38c-4cb5-9619-83e331805662",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "    <i class=\"fa fa-exclamation-triangle\"></i>&nbsp; <b>Overwritting data arrays and outlier lists</b><br>\n",
    "    Notice how when we loaded the cleaned files into a new array we just overwrote the initial array with all the files loaded but when we re-ran the outlier detection we stored the output as a new list instead of overwritting the list. This is because the <code>svd_outlier</code> function will append the list instead of overwritting it. To avoid issues caused by this bahvior, it is best to store the second round (and any subsequent iterations) of outlier detection in separate lists. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab7159c-0d7b-4a25-a164-8c82a46beb26",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Step 2.5: Remove Outliers\n",
    "Now that we have detected all the scattering curve outliers in both \"laser on\" and \"laser off\" scattering curves, we need to create a file list that has all of the outliers removed. This is itself a multi-step process. \n",
    "\n",
    "#### Step 2.5.1: Combining Outlier Lists\n",
    "First, the outlier lists need to be combined to a common list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0d1166-c60d-4a1c-ae52-7288c2b7b210",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# combine all outliers into a single list\n",
    "all_on_outliers = on_outlier_files1 + on_outlier_files2\n",
    "all_off_outliers = off_outlier_files1 + off_outlier_files2\n",
    "\n",
    "all_outliers = all_on_outliers + all_off_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d446387f-7e06-4442-8184-3aa573502837",
   "metadata": {},
   "source": [
    "#### Step 2.5.2: Removing Duplicate Outliers from Outlier Lists\n",
    "Becuase the same curve number can simultaneously be an outlier for both \"laser off\" and \"laser on\" data sets, the combined outlier lists could have duplicate curve numbers, which would cause issues with removing these files from the list. Therefore, to avoid issues further into the worflow, it is best practice to get the unique set of outlier curve numbers and sort the file list in order of collection using built in Python functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5373b6af-ddad-42c6-a17f-3dce85a948fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select only unique values for outliers\n",
    "svd_set = set(all_outliers)\n",
    "all_outliers = list(svd_set)\n",
    "\n",
    "# sort list of outliers\n",
    "all_outliers.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17514b8d-df9b-408b-a96e-daf9565f6597",
   "metadata": {},
   "source": [
    "#### Step 2.5.3: Creating a Cleaned File List\n",
    "The final step of the outlier detection process is to remove all the outliers from the file list to create a cleaned file list that can be used as input for further processing and analysis. The outlier removal follows the same procedure as the initial outlier removal step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8353cd31-86fd-4c24-91c9-006ceac6f990",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove SVD iteration 2 outliers from file lists\n",
    "on_cleaned, on_outliers = remove_outliers(flist=on_files, olist=all_outliers, fslice=[-9,-6])        \n",
    "off_cleaned, off_outliers = remove_outliers(flist=off_files, olist=all_outliers, fslice=[-9,-6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cea5e1-1191-460f-89b7-6d220d68bbf7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "    <i class=\"fa fa-exclamation-triangle\"></i>&nbsp; <b>Static SAXS Outlier Detection</b><br>\n",
    "    This function can also be used to detect the outliers in static SAXS data sets as well. In the case of static SAXS data, the workflow is simplified because, instead of paired \"laser on\" and \"laser off\" curves, there is only one set of \"laser off\" scattering curves so the analysis only has to be done on the one set. It is still recommended to use at least 2 outlier detection steps just as for the TR, T-Jump data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a3c307-ef5a-4004-a56a-256e03c51025",
   "metadata": {},
   "source": [
    "## Step 3: Calculate TR, T-Jump Difference Curves\n",
    "\n",
    "In static SAXS analysis, the scattering curves remaining after outlier removal are used in subequent analysis. However, in TR, T-Jump SAXS analysis the difference curves are used in subsequent analysis. The difference curves are determined by subtracting each \"laser off\" curve from its paired \"laser on\" curve. Calculating the difference curves is also a multi-step process because the curves must be scaled to a common point prior to subtraction. \n",
    "\n",
    "### Step 3.1: Create Labels\n",
    "The file names can be too long to use as labels so, to keep the output files names and plot labels tidy, it is recommended to create a list of labels that can be mapped to the data array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd85bd1a-81a0-432c-9bf7-f3b9bde567c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of labels to use for saving diff curves\n",
    "labs = []\n",
    "for i in on_cleaned:\n",
    "    labs.append(i[-9:-6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dadcd2-3156-42c6-93f1-a6d989ac2018",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "For creating the labels, we used the on list in the above code, but both the `on_cleaned` and `off_cleaned` files should be sorted and be in the same order. Assuming that both the `on_cleaned` and `off_cleaned` files are sorted in the same order, either list could be used to create the label list. \n",
    "\n",
    "### Step 3.2 & 3.3: Scaling and Subtracting Curves\n",
    "Because the difference curves have to be calculated for each set of paired \"laser on\" and \"laser off\" curves, these 2 steps have to be performed together within a loop. \n",
    "\n",
    "Prior to subtracting the \"laser off\" curve from its paired \"laser on\" curve the curves must be scaled to a common curve. Because the \"laser on\" data contains the scattering signal from which kinetics will later be extracted, the best practice is to scale the \"laser off\" curve to the \"laser on\" curve so that artifacts are not introduced to the \"laser on\" curves during data processing. This means that the \"laser on\" curve can be used as the reference curve for the scaling of its paired \"laser off\" curve. The range of scaling should be determined for each data set individually. \n",
    "\n",
    "The `saxs_scale` function will scale a given curve to a given reference curve. This function has several input parameters:\n",
    ">1. `ref` is a string that indicates the file (including the full path) containing the reference curve, typically the \"laser on\" curve for this analysis. <br>\n",
    "> 2. `scale` is a string that indicates the file containing the curve to be scaled, typically the paired \"laser off\" curve for this analysis. <br>\n",
    "> 3. The `dataset` parameter is a string that includes a label for the data set, which will be used for output files and plots. These three parameters are all required input. There are also several optional parameters.<br>\n",
    "> 4. `err` is a boolean indicating if the files for both the reference and scaled curves contain a column for measurement errors. If set the True, then the errors will be propagated for this step. The default value is False, which does nothing with the errors. <br>\n",
    "> 5. `delim` indicates the type of delimitter used in the referece and scaled curve files and the default value is a comma.<br>\n",
    "> 6. The `mask` parameter is an integer indicating the number of rows to skip when importing data, with the default value set to 0. This parameter can be useful when trying to load data files that have metadata or headers. the `qmin` and `qmax` values indicate the minimum and maximum, respectively, values of the scattering vector to use for the scaling range. <br>\n",
    "> 7. `outfile` and `outdir` indicate the output file name and location of output file, respectively. <br>\n",
    "> 8. The `saxs_scale` function returns the scaled curve as a numpy array. This function will also display before and after scaling plots for scaled curve. <br>\n",
    "\n",
    "It is recommended that these plots be examined closely for any issues with scaling as this is one of the trickier steps in the analysis workflow. \n",
    "\n",
    "The `saxs_sub` function will subtract the indicated curve from a given reference curve. This function has 2 required input parameters, `ref` and `data`, which are both strings containing the file name (with the full path) to the curve that will be subtracted and the curve that will be corrected, respectively. When calculating T-Jump difference curves, the reference curve is the \"laser off\" curve and the data curve is the paired \"laser on\" curve. Optional parameters include:\n",
    "> 1. `delim_ref` and `delim_data` indicate the delimitters used in the reference curve file and the data curve file, respectively. <br>\n",
    "> 2. Likewise, `ref_skip` and `data_skip` indicate the number of rows to skip when loadng the reference curve and data curve, respectively.<br>\n",
    "> 3. The `outdir` and `outfile` parameters indicate the name of the output file and where it should be saved. If the indicated output directory does not exist, the function will create it using the `make_dir()` function covered in [Tutorial 1](https://gitlab.oit.duke.edu/tr_t-jump_saxs/y22-23/-/blob/main/TUTORIALS/tutorial1_file-handling.ipynb?ref_type=heads \"Tutorial 1\").<br>\n",
    "\n",
    "The `saxs_sub` function returns a numpy array containing the corrected curve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff0dbda-b18c-4042-b97e-7c07ab38b87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, f, l in zip(on_cleaned, off_cleaned, labs):\n",
    "    # scale laser off to laser on curve at isosbestic point for water\n",
    "    scaled = saxs_scale(ref=str(n), scale=str(f), dataset='SAXS_Tutorial3_' + str(l),\n",
    "                       qmin=1.5, qmax=2.5, delim=' ', err=False, mask=10, outfile='scaled' + str(n[75:]),\n",
    "                       outdir='./OUTPUT/TUTORIAL3/LASER_OFF_SCALED/')\n",
    "\n",
    "    buf_sub = saxs_sub(ref=f, data=n, delim_ref=' ', delim_data=' ', err=False, ref_skip=10, \n",
    "                        data_skip=10, outdir='./OUTPUT/TUTORIAL3/DIFF_CURVES/', \n",
    "                        outfile= 'diff' + str(n[75:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e225387d-35d5-490e-b7c4-ffd49f849737",
   "metadata": {},
   "source": [
    "# Example 2: Iterating Over Multiple Time Delays and Sets\n",
    "You can automate the above analysis over multiple time delays to speed up processing for TR, T-Jump SAXS curves. \n",
    "\n",
    "## Step 1: Define Dataset Variables\n",
    "To iterate over multiple time delays at once, the dataset(s), time delays, file directories, and file prefixes need to be assigned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c33200a-b7f1-4155-8a9b-ceb23f5733c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# define det numbers\n",
    "sets = ['20hz_set02', '20hz_set02', '20hz_set02',\n",
    "        '20hz_set01', '20hz_set01', '20hz_set01', '20hz_set01', '20hz_set01',\n",
    "        '20hz_set03', '20hz_set03', '20hz_set03',\n",
    "        '5hz_set01', '5hz_set01', '5hz_set01']\n",
    "\n",
    "# define datasets\n",
    "time_delays = ['1.5us', '3us', '5us', \n",
    "               '10us', '50us', '100us', '500us', '1ms',\n",
    "               '5us', '300us', '1ms',\n",
    "               '1ms', '10ms', '100ms']\n",
    "\n",
    "# define data directories\n",
    "directories = ['../../../TR_T-jump_SAXS_July2022/protein_20hz_set02/processed/', '../../../TR_T-jump_SAXS_July2022/protein_20hz_set02/processed/',\n",
    "               '../../../TR_T-jump_SAXS_July2022/protein_20hz_set02/processed/', '../../../TR_T-jump_SAXS_July2022/protein_20hz_set01/processedb/',\n",
    "               '../../../TR_T-jump_SAXS_July2022/protein_20hz_set01/processedb/','../../../TR_T-jump_SAXS_July2022/protein_20hz_set01/processedb/',\n",
    "               '../../../TR_T-jump_SAXS_July2022/protein_20hz_set01/processedb/', '../../../TR_T-jump_SAXS_July2022/protein_20hz_set01/processedb/',\n",
    "               '../../../TR_T-jump_SAXS_July2022/protein_20hz_set03_redo/processed/', '../../../TR_T-jump_SAXS_July2022/protein_20hz_set03_redo/processed/',\n",
    "               '../../../TR_T-jump_SAXS_July2022/protein_20hz_set03_redo/processed/', '../../../TR_T-jump_SAXS_July2022/protein_5hz_set01/processdb/',\n",
    "               '../../../TR_T-jump_SAXS_July2022/protein_5hz_set01/processdb/', '../../../TR_T-jump_SAXS_July2022/protein_5hz_set01/processdb/'] \n",
    "\n",
    "prefixes = ['protein_20hs_44C_', 'protein_20hs_44C_', 'protein_20hs_44C_',\n",
    "           'protein_20hz_set01_','protein_20hz_set01_','protein_20hz_set01_','protein_20hz_set01_','protein_20hz_set01_',\n",
    "           'protein_20hz_44C_', 'protein_20hz_44C_', 'protein_20hz_44C_',\n",
    "           'protein_5hz_set01_', 'protein_5hz_set01_', 'protein_5hz_set01_']\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27b204a-8fff-44b6-8725-6157432fad36",
   "metadata": {},
   "source": [
    "## Step 2: Iterating Processing Over All Time Delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a28ce4-82e3-49a2-b51e-fb14554d4eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each data set\n",
    "for t,d,p,s in zip(time_delays, directories, prefixes, sets):\n",
    "    \n",
    "    print('Analyzing ' + str(t) + '..')\n",
    "    \n",
    "    # Get all laser on scattering curves\n",
    "    on_files = make_flist(directory=str(d), \n",
    "                            prefix=str(p) + str(t), suffix='_Q.chi')\n",
    "\n",
    "    # Get all -10us scattering curves from 20 Hz set 3 CH505TF Data\n",
    "    off_files = make_flist(directory=str(d),\n",
    "                            prefix=str(p), suffix='_Q.chi')\n",
    "\n",
    "    # sort files lists\n",
    "    on_files.sort()\n",
    "    off_files.sort()\n",
    "    \n",
    "\n",
    "    # load laser on scattering curves\n",
    "    on_data, on_array, q, on_err = load_set(flist=on_files, delim=' ', mask=10, err=False)\n",
    "\n",
    "    # load laser off scattering curves\n",
    "    off_data, off_array, q, off_err = load_set(flist=off_files, delim=' ', mask=10, err=False)\n",
    "    \n",
    "\n",
    "    # plot curves for sanity check\n",
    "    print('Plotting Scattering Curves...')\n",
    "\n",
    "    # Create list of labels to use for plot legend\n",
    "    labs = []\n",
    "    for i in on_files:\n",
    "        labs.append(i[-9:-6])\n",
    "\n",
    "    # plot laser on curves\n",
    "    plot_curve(data_arr=on_array, q_arr=q, labels=labs, qmin=0.025, qmax=0.15, imin=None, imax=None,\n",
    "                x='scattering vector (Å)', y='scattering intensity', title=str(s) + ' SAXS Scattering at ' + str(t), \n",
    "                save=False, save_dir=None, save_name=None)\n",
    "\n",
    "    # plot laser off curves\n",
    "    plot_curve(data_arr=off_array, q_arr=q, labels=labs, qmin=0.025, qmax=0.15, imin=None, imax=None, \n",
    "                x='scattering vector (Å)', y='scattering intensity', title=str(s) + ' SAXS Scattering at -5us', \n",
    "                save=False, save_dir=None, save_name=None)\n",
    "\n",
    "    #SVD outlier detection iteration 1\n",
    "    print('Running SVD 1...')\n",
    "    on_outlier_files1, on_outlier_index1 = svd_outliers(arr=on_array, flist=on_files, q=q, cutoff=2.5, \n",
    "                                                        save_dir='./OUTPUT/TUTORIAL3/' + str(s) + '/OUTLIERS/' + str(t) + '/', \n",
    "                                                        save_name=str(t) + '_svd_outliers1')\n",
    "\n",
    "    off_outlier_files1, off_outlier_index1 = svd_outliers(arr=off_array, flist=off_files, q=q, cutoff=2.5, \n",
    "                                                            save_dir='./OUTPUT/TUTORIAL3/' + str(s) + '/OUTLIERS/' + str(t) + '/', \n",
    "                                                            save_name=str(t) + '_-5us_svd_outliers1')\n",
    "    \n",
    "    # combine all outliers into a single list\n",
    "    all_svd_outliers = on_outlier_files1 + off_outlier_files1\n",
    "    \n",
    "    # select only unique values for outliers\n",
    "    svd_outliers1 = unique_set(lst=all_svd_outliers)\n",
    "    \n",
    "    # sort list of outliers\n",
    "    svd_outliers1.sort()\n",
    "\n",
    "    # remove SVD iteration 1 outliers from file lists\n",
    "    on_cleaned, on_outliers = remove_outliers(flist=on_files, olist=svd_outliers1, fslice=[-9,-6])        \n",
    "    off_cleaned, off_outliers = remove_outliers(flist=off_files, olist=svd_outliers1, fslice=[-9,-6])\n",
    "    \n",
    "    # load clean curves\n",
    "    on_data, on_array, q, on_err = load_set(flist=on_cleaned, delim=' ', mask=10, err=False)\n",
    "    off_data, off_array, q, off_err = load_set(flist=off_cleaned, delim=' ', mask=10, err=False)\n",
    "    \n",
    "    #SVD outlier detection iteration 2\n",
    "    print('Running SVD 2...')\n",
    "    on_outlier_files2, on_outlier_index2 = svd_outliers(arr=on_array, flist=on_cleaned, q=q, cutoff=2.5, \n",
    "                                                        save_dir='./OUTPUT/TUTORIAL3/' + str(s) + '/OUTLIERS/' + str(t) + '/', \n",
    "                                                        save_name=str(t) + '_svd_outliers2')\n",
    "\n",
    "    off_outlier_files2, off_outlier_index2 = svd_outliers(arr=off_array, flist=off_cleaned, q=q, cutoff=2.5, \n",
    "                                                            save_dir='./OUTPUT/TUTORIAL3/' + str(s) + '/OUTLIERS/' + str(t) + '/', \n",
    "                                                            save_name=str(t) + '_-5us_svd_outliers2')\n",
    "    \n",
    "    # combine all outliers into a single list\n",
    "    all_on_outliers = on_outlier_files1 + on_outlier_files2\n",
    "    all_off_outliers = off_outlier_files1 + off_outlier_files2\n",
    "    all_outliers = all_on_outliers + all_off_outliers\n",
    "    \n",
    "    # select only unique values for outliers\n",
    "    #svd_set = set(all_outliers)\n",
    "    #all_outliers = list(svd_set)\n",
    "    unique_outliers = unique_set(all_outliers)\n",
    "    \n",
    "    # sort list of outliers\n",
    "    unique_outliers.sort()\n",
    "    \n",
    "    # remove SVD iteration 2 outliers from file lists\n",
    "    on_cleaned, on_outliers = remove_outliers(flist=on_files, olist=unique_outliers, fslice=[-9,-6])        \n",
    "    off_cleaned, off_outliers = remove_outliers(flist=off_files, olist=unique_outliers, fslice=[-9,-6])\n",
    "    \n",
    "    # Create list of labels to use for saving diff curves\n",
    "    labs = []\n",
    "    for i in on_cleaned:\n",
    "        labs.append(i[-9:-6])\n",
    "    \n",
    "    # loop over all laser on and laser off files\n",
    "    for n, f, l in zip(on_cleaned, off_cleaned, labs):\n",
    "        # scale laser off to laser on curve at isosbestic point for water\n",
    "        scaled = saxs_scale(ref=str(n), scale=str(f), dataset=str(s) + '_' + str(t) + '_' + str(l) + '_-5us',\n",
    "                            qmin=1.4, qmax=1.6, delim=' ', err=False, mask=10, \n",
    "                            outfile=str(s) + '_' + str(n[-9:-6]) + '_Q.chi',\n",
    "                            outdir='./OUTPUT/TUTORIAL3/' + str(s) + '/LASER_OFF_SCALE/' + str(t))\n",
    "        \n",
    "        # calculate difference curve\n",
    "        buf_sub = saxs_sub(ref=f, \n",
    "                           data='./OUTPUT/TUTORIAL3/' + str(s) + '/LASER_OFF_SCALE/' + str(t) + '/' + str(s) + '_' + str(n[-9:-6]) + '_Q.chi',\n",
    "                           delim_ref=' ', delim_data=',', err=False, ref_skip=10, \n",
    "                            data_skip=10, outdir='./OUTPUT/TUTORIAL3/' + str(s) + '/-5us_DIFF/' + str(t) + '/', \n",
    "                            outfile= str(s) + '_' + str(n[-9:-6]) + '_Q.chi')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d8753a-0507-4116-8682-2d0cb3107487",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "    <i class=\"fa fa-check-circle\"></i>&nbsp; <b>Congratulations!</b><br>\n",
    "    You completed the third tutorial! Continue with <a href=\"https://github.com/ScientistAsh/tr_tjump_saxs/blob/main/TUTORIALS/tutorial4_saxs_processing_difference.ipynb\">Tutorial 4</a> to continue the analysis with processing T-Jump difference curves.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a9c0e4-480b-4702-b6ee-dd895e0eaa41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
